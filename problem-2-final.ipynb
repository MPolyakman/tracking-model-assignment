{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание №2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая терминология по используемым данным\n",
    "\n",
    "Предоставляемые данные для разработки моделей и алгоритмов трекинга мяча в теннисе представляют собор набор игр (game), состоящих из нескольких клипов (clip), каждый из которых состоит из набора кадров (frame). Обратите внимание на структуру организации файлов внутри предоставляемого датасета для полного понимания.\n",
    "\n",
    "Большинство алгоритмов трекинга объектов работают с несколькими последовательными кадрами, и в данном задании также подразумевается использование этого приема. Последовательность нескольких кадров будем именовать стопкой (stack), размер стопки (stack_s) является гиперпараметром разрабатываемого алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заготовка решения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка датасета\n",
    "Для работы с данными в ноутбуке kaggle необходимо подключить датасет. File -> Add or upload data, далее в поиске написать tennis-tracking-assignment и выбрать датасет. Если поиск не работает, то можно добавить датасет по url: https://www.kaggle.com/xubiker/tennistrackingassignment. После загрузки данные датасета будут примонтированы в ../input/tennistrackingassignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка и импорт зависимостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка необходимых пакетов (не забудьте \"включить интернет\" в настройках ноутбука kaggle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:43.169178Z",
     "iopub.status.busy": "2025-12-21T15:23:43.168766Z",
     "iopub.status.idle": "2025-12-21T15:23:50.318018Z",
     "shell.execute_reply": "2025-12-21T15:23:50.316478Z",
     "shell.execute_reply.started": "2025-12-21T15:23:43.169151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install moviepy --no-deps\n",
    "# !pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:50.320894Z",
     "iopub.status.busy": "2025-12-21T15:23:50.320428Z",
     "iopub.status.idle": "2025-12-21T15:23:50.333162Z",
     "shell.execute_reply": "2025-12-21T15:23:50.332007Z",
     "shell.execute_reply.started": "2025-12-21T15:23:50.320855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 2.2.6\n",
      "matplotlib: 3.10.7\n",
      "moviepy: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import moviepy\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"matplotlib:\", plt.matplotlib.__version__)\n",
    "print(\"moviepy:\", moviepy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт необходимых зависимостей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:50.334431Z",
     "iopub.status.busy": "2025-12-21T15:23:50.334126Z",
     "iopub.status.idle": "2025-12-21T15:23:50.929881Z",
     "shell.execute_reply": "2025-12-21T15:23:50.929099Z",
     "shell.execute_reply.started": "2025-12-21T15:23:50.334406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Sequence\n",
    "\n",
    "import numpy as np\n",
    "from numpy import unravel_index\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm, notebook\n",
    "\n",
    "from moviepy.video.io.ImageSequenceClip import ImageSequenceClip\n",
    "\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Набор функций для загрузки данных из датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция load_clip_data загружает выбранный клип из выбранной игры и возвращает его в виде numpy массива [n_frames, height, width, 3] типа uint8. Для ускорения загрузки используется кэширование - однажды загруженные клипы хранятся на диске в виде npz архивов, при последующем обращении к таким клипам происходит загрузка npz архива.\n",
    "\n",
    "<font color=\"red\">\n",
    "Также добавлена возможность чтения клипа в половинном разрешении 640x360, вместо оригинального 1280x720 для упрощения и ускорения разрабатываемых алгоритмов.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция load_clip_labels загружает референсные координаты мяча в клипе в виде numpy массива [n_frames, 4], где в каждой строке массива содержатся значения [code, x, y, q]. x, y соответствуют координате центра мяча на кадре, q не используется в данном задании, code описывает статус мяча:\n",
    "* code = 0 - мяча в кадре нет\n",
    "* code = 1 - мяч присутствует в кадре и легко идентифицируем\n",
    "* code = 2 - мяч присутствует в кадре, но сложно идентифицируем\n",
    "* code = 3 - мяч присутствует в кадре, но заслонен другими объектами.\n",
    "\n",
    "При загрузке в половинном разрешении координаты x, y делятся на 2.\n",
    "\n",
    "Функция load_clip загружает выбранный клип и соответствующий массив координат и возвращает их в виде пары."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:50.931105Z",
     "iopub.status.busy": "2025-12-21T15:23:50.930571Z",
     "iopub.status.idle": "2025-12-21T15:23:50.948405Z",
     "shell.execute_reply": "2025-12-21T15:23:50.947384Z",
     "shell.execute_reply.started": "2025-12-21T15:23:50.931077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_num_clips(path: Path, game: int) -> int:\n",
    "    return len(list((path / f'game{game}/').iterdir()))\n",
    "\n",
    "\n",
    "def get_game_clip_pairs(path: Path, games: List[int]) -> List[Tuple[int, int]]:\n",
    "    return [(game, c)  for game in games for c in range(1, get_num_clips(path, game) + 1)]\n",
    "\n",
    "\n",
    "def load_clip_data(path: Path, game: int, clip: int, downscale: bool, quiet=False) -> np.ndarray:\n",
    "    if not quiet:\n",
    "        suffix = 'downscaled' if downscale else ''\n",
    "        print(f'loading clip data (game {game}, clip {clip}) {suffix}')\n",
    "    cache_path = path / 'cache'\n",
    "    cache_path.mkdir(exist_ok=True)\n",
    "    resize_code = '_ds2' if downscale else ''\n",
    "    cached_data_name = f'{game}_{clip}{resize_code}.npz'\n",
    "    if (cache_path / cached_data_name).exists():\n",
    "        clip_data = np.load(cache_path / cached_data_name)['clip_data']\n",
    "    else:\n",
    "        clip_path = path / f'game{game}/clip{clip}'\n",
    "        n_imgs = len(list(clip_path.iterdir())) - 1\n",
    "        imgs = [None] * n_imgs\n",
    "        for i in notebook.tqdm(range(n_imgs)):\n",
    "            img = Image.open(clip_path / f'{i:04d}.jpg')\n",
    "            if downscale:\n",
    "                img = img.resize((img.width // 2, img.height // 2),)\n",
    "            imgs[i] = np.array(img, dtype=np.uint8)\n",
    "        clip_data = np.stack(imgs)\n",
    "        cache_path.mkdir(exist_ok=True, parents=True)\n",
    "        np.savez_compressed(cache_path / cached_data_name, clip_data=clip_data)\n",
    "    return clip_data\n",
    "\n",
    "\n",
    "def load_clip_labels(path: Path, game: int, clip: int, downscale: bool, quiet=False):\n",
    "    if not quiet:\n",
    "        print(f'loading clip labels (game {game}, clip {clip})')\n",
    "    clip_path = path / f'game{game}/clip{clip}'\n",
    "    labels = []\n",
    "    with open(clip_path / 'labels.csv') as csvfile:\n",
    "        lines = list(csv.reader(csvfile))\n",
    "        for line in lines[1:]:\n",
    "            values = np.array([-1 if i == '' else int(i) for i in line[1:]])\n",
    "            if downscale:\n",
    "                values[1] //= 2\n",
    "                values[2] //= 2\n",
    "            labels.append(values)\n",
    "    return np.stack(labels)\n",
    "\n",
    "\n",
    "def load_clip(path: Path, game: int, clip: int, downscale: bool, quiet=False):\n",
    "    data = load_clip_data(path, game, clip, downscale, quiet)\n",
    "    labels = load_clip_labels(path, game, clip, downscale, quiet)\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Набор дополнительных функций\n",
    "\n",
    "Еще несколько функций, немного облегчающих выполнение задания:\n",
    "\n",
    "* prepare_expariment создает новую директорию в out_path для хранения результатов текущего эксперимента. Нумерация выполняется автоматически, функция возвращает путь к созданной директории эксперимента;\n",
    "* ball_gauss_template - создает \"шаблон\" мяча, может быть использована в алгоритмах поиска мяча на изображении по корреляции;\n",
    "* create_masks - принимает набор кадров и набор координат мяча, и генерирует набор масок, в которых помещает шаблон мяча на заданные координаты. Может быть использована при обучении нейронной сети семантической сегментации;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "augmentation = transforms.ColorJitter(\n",
    "    hue=0.25,\n",
    "    saturation=0.5 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:50.950959Z",
     "iopub.status.busy": "2025-12-21T15:23:50.950549Z",
     "iopub.status.idle": "2025-12-21T15:23:50.976552Z",
     "shell.execute_reply": "2025-12-21T15:23:50.975207Z",
     "shell.execute_reply.started": "2025-12-21T15:23:50.950935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_experiment(out_path: Path) -> Path:\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    dirs = [d for d in out_path.iterdir() if d.is_dir() and d.name.startswith('exp_')]\n",
    "    experiment_id = max(int(d.name.split('_')[1]) for d in dirs) + 1 if dirs else 1\n",
    "    exp_path = out_path / f'exp_{experiment_id}'\n",
    "    exp_path.mkdir()\n",
    "    return exp_path\n",
    "\n",
    "\n",
    "def ball_gauss_template(rad, sigma):\n",
    "    x, y = np.meshgrid(np.linspace(-rad, rad, 2 * rad + 1), np.linspace(-rad, rad, 2 * rad + 1)) \n",
    "    dst = np.sqrt(x * x + y * y) \n",
    "    gauss = np.exp(-(dst ** 2 / (2.0 * sigma ** 2)))     \n",
    "    return gauss\n",
    "\n",
    "\n",
    "def create_masks(data: np.ndarray, labels: np.ndarray, resize):\n",
    "    rad = 64 #25\n",
    "    sigma = 14\n",
    "    if resize:\n",
    "        rad //= 2\n",
    "    ball = ball_gauss_template(rad, sigma)\n",
    "    n_frames = data.shape[0]\n",
    "    sh = rad\n",
    "    masks = []\n",
    "    for i in range(n_frames):\n",
    "        label = labels[i, ...] \n",
    "        frame = data[i, ...]\n",
    "        if 0 < label[0] < 3:\n",
    "            x, y = label[1:3]\n",
    "            mask = np.zeros((frame.shape[0] + 2 * rad + 2 * sh, frame.shape[1] + 2 * rad + 2 * sh), np.float32)\n",
    "            mask[y + sh : y + sh + 2 * rad + 1, x + sh : x + sh + 2 * rad + 1] = ball\n",
    "            mask = mask[rad + sh : -rad - sh, rad + sh : -rad - sh]\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            masks.append(np.zeros((frame.shape[0], frame.shape[1]), dtype=np.float32))\n",
    "    return np.stack(masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Набор функций, предназначенных для визуализации результатов\n",
    "\n",
    "Функция visualize_prediction принимает набор кадров, набор координат детекции мяча (можно подавать как референсные значения, так и предсказанные) и создает видеоклип, в котором отрисовывается положение мяча, его трек, номер кадра и метрика качества трекинга (если она была передана в функцию). Видеоклип сохраняется в виде mp4 файла. Кроме того данная функция создает текстовый файл, в который записывает координаты детекции мяча и значения метрики качества трекинга.\n",
    "\n",
    "Функция visualize_prob принимает набор кадров и набор предсказанных карт вероятности и создает клип с наложением предсказанных карт вероятности на исходные карты. Области \"подсвечиваются\" желтым, клип сохраняется в виде mp4 видеофайла. Данная функция может быть полезна при наличии в алгоритме трекинга сети, осуществляющей семантическую сегментацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:50.978010Z",
     "iopub.status.busy": "2025-12-21T15:23:50.977571Z",
     "iopub.status.idle": "2025-12-21T15:23:51.005981Z",
     "shell.execute_reply": "2025-12-21T15:23:51.004833Z",
     "shell.execute_reply.started": "2025-12-21T15:23:50.977981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using font: C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\mpl-data\\fonts\\ttf\\DejaVuSans.ttf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "from pathlib import Path\n",
    "\n",
    "# Find first available TTF font from matplotlib\n",
    "font_path = Path(fm.findfont('DejaVu Sans'))\n",
    "print(f\"Using font: {font_path}\")\n",
    "\n",
    "def _add_frame_number(frame: np.ndarray, number: int) -> np.ndarray:\n",
    "    try:\n",
    "        fnt = ImageFont.truetype(str(font_path), 30)\n",
    "    except OSError:\n",
    "        fnt = ImageFont.load_default()\n",
    "    img = Image.fromarray(frame)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((10, 10), f'frame {number}', font=fnt, fill=(255, 0, 255))\n",
    "    return np.array(img)\n",
    "\n",
    "\n",
    "def _vis_clip(data: np.ndarray, lbls: np.ndarray, metrics: List[float] = None, ball_rad=5, color=(255, 0, 0), track_length=10):\n",
    "    print('perfoming clip visualization')\n",
    "    n_frames = data.shape[0]\n",
    "    frames_res = []\n",
    "    try:\n",
    "        fnt = ImageFont.truetype(str(font_path), 30)\n",
    "    except OSError:\n",
    "        fnt = ImageFont.load_default()\n",
    "    for i in range(n_frames):\n",
    "        img = Image.fromarray(data[i, ...])\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        txt = f'frame {i}'\n",
    "        if metrics is not None:\n",
    "            txt += f', SiBaTrAcc: {metrics[i]:.3f}'\n",
    "        draw.text((10, 10), txt, font=fnt, fill=(255, 0, 255))\n",
    "        label = lbls[i]\n",
    "        if label[0] != 0: # the ball is clearly visible\n",
    "            px, py = label[1], label[2]\n",
    "            draw.ellipse((px - ball_rad, py - ball_rad, px + ball_rad, py + ball_rad), outline=color, width=2)\n",
    "            for q in range(track_length):\n",
    "                if lbls[i-q-1][0] == 0:\n",
    "                    break\n",
    "                if i - q > 0:\n",
    "                    draw.line((lbls[i - q - 1][1], lbls[i - q - 1][2], lbls[i - q][1], lbls[i - q][2]), fill=color)                \n",
    "        frames_res.append(np.array(img))\n",
    "    return frames_res\n",
    "\n",
    "\n",
    "def _save_clip(frames: Sequence[np.ndarray], path: Path, fps):\n",
    "    assert path.suffix in ('.mp4', '.gif')\n",
    "    clip = ImageSequenceClip(frames, fps=fps)\n",
    "    if path.suffix == '.mp4':\n",
    "        clip.write_videofile(str(path), fps=fps, logger=None)\n",
    "    else:\n",
    "        clip.write_gif(str(path), fps=fps, logger=None)\n",
    "\n",
    "\n",
    "def _to_yellow_heatmap(frame: np.ndarray, pred_frame: np.ndarray, alpha=0.4):\n",
    "    img = Image.fromarray((frame * alpha).astype(np.uint8))\n",
    "    maskR = (pred_frame * (1 - alpha) * 255).astype(np.uint8)\n",
    "    maskG = (pred_frame * (1 - alpha) * 255).astype(np.uint8)\n",
    "    maskB = np.zeros_like(maskG, dtype=np.uint8)\n",
    "    mask = np.stack([maskR, maskG, maskB], axis=-1)\n",
    "    return img + mask\n",
    "\n",
    "\n",
    "def _vis_pred_heatmap(data_full: np.ndarray, pred_prob: np.ndarray, display_frame_number):\n",
    "    n_frames = data_full.shape[0]\n",
    "    v_frames = []\n",
    "    for i in range(n_frames):\n",
    "        frame = data_full[i, ...]\n",
    "        pred = pred_prob[i, ...]\n",
    "        hm = _to_yellow_heatmap(frame, pred)\n",
    "        if display_frame_number:\n",
    "            hm = _add_frame_number(hm, i)\n",
    "        v_frames.append(hm)\n",
    "    return v_frames\n",
    "\n",
    "\n",
    "def visualize_prediction(data_full: np.ndarray, labels_pr: np.ndarray, save_path: Path, name: str, metrics=None, fps=15):\n",
    "    with open(save_path / f'{name}.txt', mode='w') as f:\n",
    "        if metrics is not None:\n",
    "            f.write(f'SiBaTrAcc: {metrics[-1]} \\n')\n",
    "        for i in range(labels_pr.shape[0]):\n",
    "            f.write(f'frame {i}: {labels_pr[i, 0]}, {labels_pr[i, 1]}, {labels_pr[i, 2]} \\n')\n",
    "\n",
    "    v = _vis_clip(data_full, labels_pr, metrics)\n",
    "    _save_clip(v, save_path / f'{name}.mp4', fps=fps)\n",
    "\n",
    "\n",
    "def visualize_prob(data: np.ndarray, pred_prob: np.ndarray, save_path: Path, name: str, frame_number=True, fps=15):\n",
    "    v_pred = _vis_pred_heatmap(data, pred_prob, frame_number)\n",
    "    _save_clip(v_pred, save_path / f'{name}_prob.mp4', fps=fps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс DataGenerator \n",
    "\n",
    "Класс, отвечающий за генерацию данных для обучения модели. Принимает на вход путь к директории с играми, индексы игр, используемые для генерации данных, и размер стопки. Хранит в себе автоматически обновляемый пул с клипами игр.\n",
    "\n",
    "В пуле содержится pool_s клипов. DataGenerator позволяет генерировать батч из стопок (размера stack_s) последовательных кадров. Выбор клипа для извлечения данных взвешенно-случайный: чем больше длина клипа по сравнению с другими клипами в пуле, тем вероятнее, что именно из него будет сгенерирована стопка кадров. Выбор стопки кадров внтури выбранного клипа полностью случаен. Кадры внутри стопки конкатенируются по последнему измерению (каналам).\n",
    "\n",
    "После генерирования количества кадров равного общему количеству кадров, хранимых в пуле, происходит автоматическое обновление пула: из пула извлекаются pool_update_s случайных клипов, после чего в пул загружается pool_update_s случайных клипов, не присутствующих в пуле. В случае, если размер пула pool_s больше или равен суммарному количеству клипов в играх, переданных в конструктор, все клипы сразу загружаются в пул, и автообновление не производится.\n",
    "\n",
    "Использование подобного пула позволяет работать с практически произвольным количеством клипов, без необходимости загружать их всех в оперативную память."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вашего удобства функция извлечения стопки кадров из пула помимо самой стопки также создает и возвращает набор сгенерированных масок с мячом исходя из референсных координат мяча в клипе.\n",
    "\n",
    "Функция random_g принимает гиперпараметр размера стопки кадров и предоставляет генератор, возвращающий стопки кадров и соответствующие им маски. Данный генератор может быть использован при реализации решения на tensorflow. Обновление пула происходит автоматически, об этом беспокоиться не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:51.007371Z",
     "iopub.status.busy": "2025-12-21T15:23:51.006975Z",
     "iopub.status.idle": "2025-12-21T15:23:51.032427Z",
     "shell.execute_reply": "2025-12-21T15:23:51.031136Z",
     "shell.execute_reply.started": "2025-12-21T15:23:51.007347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "\n",
    "    def __init__(self, path: Path, games: List[int], stack_s, downscale, pool_s=30, pool_update_s=10, pool_autoupdate=True, quiet=False) -> None:\n",
    "        self.path = path\n",
    "        self.stack_s = stack_s\n",
    "        self.downscale = downscale\n",
    "        self.pool_size = pool_s\n",
    "        self.pool_update_size = pool_update_s\n",
    "        self.pool_autoupdate = pool_autoupdate\n",
    "        self.quiet = quiet\n",
    "        self.data = []\n",
    "        self.masks = []\n",
    "\n",
    "        self.frames_in_pool = 0\n",
    "        self.produced_frames = 0\n",
    "        self.game_clip_pairs = get_game_clip_pairs(path, list(set(games)))\n",
    "        self.game_clip_pairs_loaded = []\n",
    "        self.game_clip_pairs_not_loaded = list.copy(self.game_clip_pairs) \n",
    "        self.pool = {}\n",
    "\n",
    "        self._first_load()\n",
    "\n",
    "    def _first_load(self):\n",
    "        # --- if all clips can be placed into pool at once, there is no need to refresh pool at all ---\n",
    "        if len(self.game_clip_pairs) <= self.pool_size:\n",
    "            for gcp in self.game_clip_pairs:\n",
    "                self._load(gcp)\n",
    "            self.game_clip_pairs_loaded = list.copy(self.game_clip_pairs)\n",
    "            self.game_clip_pairs_not_loaded.clear()\n",
    "            self.pool_autoupdate = False\n",
    "        else:\n",
    "            self._load_to_pool(self.pool_size)        \n",
    "        self._update_clip_weights()\n",
    "\n",
    "    def _load(self, game_clip_pair):\n",
    "        game, clip = game_clip_pair\n",
    "        data, labels = load_clip(self.path, game, clip, self.downscale, quiet=self.quiet)\n",
    "        masks = create_masks(data, labels, self.downscale)\n",
    "        weight = data.shape[0] if data.shape[0] >= self.stack_s else 0\n",
    "        self.pool[game_clip_pair] = (data, labels, masks, weight)\n",
    "        self.frames_in_pool += data.shape[0] - self.stack_s + 1\n",
    "        # print(f'items in pool: {len(self.pool)} - {self.pool.keys()}')\n",
    "\n",
    "    def _remove(self, game_clip_pair):\n",
    "        value = self.pool.pop(game_clip_pair)\n",
    "        self.frames_in_pool -= value[0].shape[0] - self.stack_s + 1\n",
    "        del value\n",
    "        # print(f'items in pool: {len(self.pool)} - {self.pool.keys()}')\n",
    "\n",
    "    def _update_clip_weights(self):\n",
    "        weights = [self.pool[pair][-1] for pair in self.game_clip_pairs_loaded]\n",
    "        tw = sum(weights)\n",
    "        self.clip_weights = [w / tw for w in weights]\n",
    "        # print(f'clip weights: {self.clip_weights}')\n",
    "\n",
    "    def _remove_from_pool(self, n):\n",
    "        # --- remove n random clips from pool ---\n",
    "        if len(self.game_clip_pairs_loaded) >= n:\n",
    "            remove_pairs = random.sample(self.game_clip_pairs_loaded, n)\n",
    "            for pair in remove_pairs:\n",
    "                self._remove(pair)\n",
    "                self.game_clip_pairs_loaded.remove(pair)\n",
    "                self.game_clip_pairs_not_loaded.append(pair)\n",
    "            gc.collect()\n",
    "\n",
    "    def _load_to_pool(self, n):\n",
    "        # --- add n random clips to pool ---\n",
    "        gc.collect()\n",
    "        add_pairs = random.sample(self.game_clip_pairs_not_loaded, n)\n",
    "        for pair in add_pairs:\n",
    "            self._load(pair)\n",
    "            self.game_clip_pairs_not_loaded.remove(pair)\n",
    "            self.game_clip_pairs_loaded.append(pair)\n",
    "\n",
    "    def update_pool(self):\n",
    "        self._remove_from_pool(self.pool_update_size)\n",
    "        self._load_to_pool(self.pool_update_size)\n",
    "        self._update_clip_weights()\n",
    "\n",
    "    def get_random_stack(self):\n",
    "        pair_idx = np.random.choice(len(self.game_clip_pairs_loaded), 1, p=self.clip_weights)[0]\n",
    "        game_clip_pair = self.game_clip_pairs_loaded[pair_idx]\n",
    "        d, labels, m, _ = self.pool[game_clip_pair]\n",
    "\n",
    "        start = np.random.choice(d.shape[0] - self.stack_s, 1)[0]\n",
    "\n",
    "        frames_stack = d[start : start + self.stack_s, ...]\n",
    "        frames_stack = np.squeeze(np.split(frames_stack, indices_or_sections=self.stack_s, axis=0))\n",
    "        frames_stack = np.concatenate(frames_stack, axis=-1)\n",
    "\n",
    "        mask = m[start + self.stack_s - 1, ...]\n",
    "\n",
    "        idx = start + self.stack_s - 1\n",
    "        label = labels[idx]\n",
    "        return frames_stack, mask, label\n",
    "\n",
    "    def get_random_batch(self, batch_s):\n",
    "        imgs, masks, labels = [], [], []\n",
    "        while len(imgs) < batch_s:\n",
    "            frames_stack, mask, label = self.get_random_stack()\n",
    "            imgs.append(frames_stack)\n",
    "            masks.append(mask)\n",
    "            labels.append(label)\n",
    "        if self.pool_autoupdate:\n",
    "            self.produced_frames += batch_s\n",
    "            # print(f'produced frames: {self.produced_frames} from {self.frames_in_pool}')\n",
    "            if self.produced_frames >= self.frames_in_pool:\n",
    "                self.update_pool()\n",
    "                self.produced_frames = 0\n",
    "        return np.stack(imgs), np.stack(masks), np.stack(labels)\n",
    "\n",
    "    def random_g(self, batch_s):\n",
    "        while True:\n",
    "            imgs_batch, masks_batch, labels_batch = self.get_random_batch(batch_s)\n",
    "            yield imgs_batch, masks_batch, labels_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:51.033950Z",
     "iopub.status.busy": "2025-12-21T15:23:51.033492Z",
     "iopub.status.idle": "2025-12-21T15:23:55.877001Z",
     "shell.execute_reply": "2025-12-21T15:23:55.876126Z",
     "shell.execute_reply.started": "2025-12-21T15:23:51.033908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:55.878349Z",
     "iopub.status.busy": "2025-12-21T15:23:55.877948Z",
     "iopub.status.idle": "2025-12-21T15:23:55.886224Z",
     "shell.execute_reply": "2025-12-21T15:23:55.885312Z",
     "shell.execute_reply.started": "2025-12-21T15:23:55.878323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.transforms.functional import hflip\n",
    "class PyTorchWrap(TorchDataset):\n",
    "\n",
    "    def __init__(self, base_dataset, mode= \"eval\"):\n",
    "        self.generator = base_dataset\n",
    "        self._length = 10_000_000\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        frames_stack, mask, labels = self.generator.get_random_stack()\n",
    "        \n",
    "        frames_stack = frames_stack.astype(np.float32) / 255.0\n",
    "            \n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        img_tensor = torch.from_numpy(frames_stack).permute(2, 0, 1)\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "        v_tensor = torch.tensor(labels[0], dtype=torch.long)\n",
    "\n",
    "        # if self.mode == \"train\" and random.random() > 0.75:\n",
    "        #     img_tensor = augmentation(img_tensor)\n",
    "        if self.mode == \"train\" and random.random() < 0.25:\n",
    "            img_tensor = hflip(img_tensor)\n",
    "\n",
    "        return img_tensor, mask_tensor, v_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример использования DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекомендованный размер пула pool_s=10 в случае использования уменьшенных вдвое изображений. При большем размере пула есть большая вероятность нехватки имеющихся 13G оперативной памяти.\n",
    "Используйте параметр quiet=True в конструкторе DataGenerator, если хотите скрыть все сообщения о чтении данных и обновлении пула."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:23:55.889194Z",
     "iopub.status.busy": "2025-12-21T15:23:55.887354Z",
     "iopub.status.idle": "2025-12-21T15:24:06.618932Z",
     "shell.execute_reply": "2025-12-21T15:24:06.618046Z",
     "shell.execute_reply.started": "2025-12-21T15:23:55.889165Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading clip data (game 3, clip 1) downscaled\n",
      "loading clip labels (game 3, clip 1)\n",
      "loading clip data (game 2, clip 6) downscaled\n",
      "loading clip labels (game 2, clip 6)\n",
      "loading clip data (game 3, clip 5) downscaled\n",
      "loading clip labels (game 3, clip 5)\n",
      "loading clip data (game 1, clip 3) downscaled\n",
      "loading clip labels (game 1, clip 3)\n",
      "loading clip data (game 4, clip 13) downscaled\n",
      "loading clip labels (game 4, clip 13)\n",
      "loading clip data (game 4, clip 14) downscaled\n",
      "loading clip labels (game 4, clip 14)\n",
      "loading clip data (game 2, clip 9) downscaled\n",
      "loading clip labels (game 2, clip 9)\n",
      "loading clip data (game 1, clip 2) downscaled\n",
      "loading clip labels (game 1, clip 2)\n",
      "loading clip data (game 2, clip 2) downscaled\n",
      "loading clip labels (game 2, clip 2)\n",
      "loading clip data (game 4, clip 3) downscaled\n",
      "loading clip labels (game 4, clip 3)\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n",
      "(4, 360, 640, 24) uint8 (4, 360, 640) float32\n"
     ]
    }
   ],
   "source": [
    "stack_s = 8\n",
    "batch_s = 4\n",
    "train_gen = DataGenerator(Path('data/train'), [1, 2, 3, 4], stack_s=stack_s, downscale=True, pool_s=10, pool_update_s=4, quiet=False)\n",
    "for i in range(10):\n",
    "    imgs, masks, _ = train_gen.get_random_batch(batch_s)\n",
    "    print(imgs.shape, imgs.dtype, masks.shape, masks.dtype)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:24:06.620359Z",
     "iopub.status.busy": "2025-12-21T15:24:06.619923Z",
     "iopub.status.idle": "2025-12-21T15:24:23.372663Z",
     "shell.execute_reply": "2025-12-21T15:24:23.371302Z",
     "shell.execute_reply.started": "2025-12-21T15:24:06.620330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading clip data (game 1, clip 1) downscaled\n",
      "loading clip labels (game 1, clip 1)\n",
      "loading clip data (game 1, clip 2) downscaled\n",
      "loading clip labels (game 1, clip 2)\n",
      "loading clip data (game 1, clip 3) downscaled\n",
      "loading clip labels (game 1, clip 3)\n",
      "loading clip data (game 1, clip 4) downscaled\n",
      "loading clip labels (game 1, clip 4)\n",
      "loading clip data (game 1, clip 5) downscaled\n",
      "loading clip labels (game 1, clip 5)\n",
      "loading clip data (game 1, clip 6) downscaled\n",
      "loading clip labels (game 1, clip 6)\n",
      "loading clip data (game 1, clip 7) downscaled\n",
      "loading clip labels (game 1, clip 7)\n",
      "loading clip data (game 1, clip 8) downscaled\n",
      "loading clip labels (game 1, clip 8)\n",
      "(360, 640, 24) (360, 640)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stack_s = 8\n",
    "train_gen = DataGenerator(Path('data/test'), [1], stack_s=stack_s, downscale=True, pool_s=10, pool_update_s=4, quiet=False)\n",
    "stack, mask, _ = train_gen.get_random_stack()\n",
    "print(stack.shape, mask.shape)\n",
    "\n",
    "# for i in range(stack_s):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(stack[:, :, 3 * i: 3 * i + 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс Metrics\n",
    "Класс для вычисления метрики качества трекинга SiBaTrAcc. Функция evaluate_predictions принимает массив из референсных и предсказанных координат мяча для клипа и возвращает массив аккумулированных значений SiBaTrAcc (может быть полезно для визуализации результатов предсказания) и итоговое значение метрики SiBaTrAcc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:24:23.374203Z",
     "iopub.status.busy": "2025-12-21T15:24:23.373804Z",
     "iopub.status.idle": "2025-12-21T15:24:23.383543Z",
     "shell.execute_reply": "2025-12-21T15:24:23.382227Z",
     "shell.execute_reply.started": "2025-12-21T15:24:23.374167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    def position_error(label_gt: np.ndarray, label_pr: np.ndarray, step=8, alpha=1.5, e1=5, e2=5):\n",
    "        # gt codes:\n",
    "        # 0 - the ball is not within the image\n",
    "        # 1 - the ball can easily be identified\n",
    "        # 2 - the ball is in the frame, but is not easy to identify\n",
    "        # 3 - the ball is occluded\n",
    "        if label_gt[0] != 0 and label_pr[0] == 0:\n",
    "            return e1\n",
    "        if label_gt[0] == 0 and label_pr[0] != 0:\n",
    "            return e2\n",
    "        dist = math.sqrt((label_gt[1] - label_pr[1]) ** 2 + (label_gt[2] - label_pr[2]) ** 2)\n",
    "        pe = math.floor(dist / step) ** alpha\n",
    "        pe = min(pe, 5)\n",
    "        return pe\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_predictions(labels_gt, labels_pr) -> Tuple[List[float], float]:\n",
    "        pe = [Metrics.position_error(labels_gt[i, ...], labels_pr[i, ...]) for i in range(len(labels_gt))]\n",
    "        SIBATRACC = []\n",
    "        for i, _ in enumerate(pe):\n",
    "            SIBATRACC.append(1 - sum(pe[: i + 1]) / ((i + 1) * 5))\n",
    "        SIBATRACC_total = 1 - sum(pe) / (len(labels_gt) * 5)\n",
    "        return SIBATRACC, SIBATRACC_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основной класс модели SuperTrackingModel\n",
    "\n",
    "Реализует всю логику обучения, сохранения, загрузки и тестирования разработанной модели трекинга. Этот класс можно и нужно расширять.\n",
    "\n",
    "В качестве примера вам предлагается заготовка модели, в которой трекинг осуществляется за счет предсказания маски по входному батчу и последующему предсказанию координат мяча по полученной маски. В данном варианте вызов функции предсказания координат по клипу (predict) повлечет за собой разбиение клипа на батчи, вызов предсказания маски для каждого батча, склеивание результатов в последовательность масок, вызов функции по вычислению координат мяча по маскам и возвращения результата. Описанные действия уже реализованы, вам остается только написать функции predict_on_bath и get_labels_from_prediction. Эта же функция predict используется и в вызове функции test, дополнительно вычисляя метрику качества трекинга и при необходимости визуализируя результат тестирования. Обратите внимание, что в результирующем numpy массиве с координатами помимо значений x и y первым значением в каждой строке должно идти значение code (0, если мяча в кадре нет и > 0, если мяч в кадре есть) для корректного вычисления качества трекинга.\n",
    "\n",
    "<font color=\"red\">\n",
    "Вам разрешается менять логику работы класса модели, (например, если решение не подразумевает использование масок), но при этом логика и работа функций load и test должна остаться неизменной!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-bce)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * bce).mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def forward(self, probs, targets, smooth=1.0):\n",
    "        probs = probs.view(probs.size(0), -1)\n",
    "        targets = targets.view(targets.size(0), -1)\n",
    "        intersection = (probs * targets).sum(1)\n",
    "        dice = 1 - (2 * intersection + smooth) / (probs.sum(1) + targets.sum(1) + smooth)\n",
    "        return dice.mean()\n",
    "    \n",
    "def area_regularization(mask, target_ratio=0.005):\n",
    "    area = mask.mean()\n",
    "    return (area - target_ratio).abs()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:24:23.385488Z",
     "iopub.status.busy": "2025-12-21T15:24:23.384635Z",
     "iopub.status.idle": "2025-12-21T15:24:23.414347Z",
     "shell.execute_reply": "2025-12-21T15:24:23.413033Z",
     "shell.execute_reply.started": "2025-12-21T15:24:23.385457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Helper block: Conv -> BN -> ReLU -> Conv -> BN -> ReLU\"\"\"\n",
    "    def __init__(self, in_c, out_c, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, 3, padding=dilation, dilation=dilation),\n",
    "            nn.GroupNorm(8, out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=dilation, dilation=dilation),\n",
    "            nn.GroupNorm(8, out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class SuperTrackingModel(nn.Module):\n",
    "    def __init__(self, batch_s, stack_s, out_path, downscale, threshold):\n",
    "        super().__init__()\n",
    "        self.batch_s = batch_s\n",
    "        self.stack_s = stack_s\n",
    "        self.out_path = out_path\n",
    "        self.downscale = downscale\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        input_channels = 3 * stack_s \n",
    "\n",
    "        self.enc1 = ConvBlock(input_channels, 32)\n",
    "        self.pool1 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        self.enc2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n",
    "\n",
    "        self.bottleneck = ConvBlock(64, 128)\n",
    "\n",
    "        # Up 1: 1/4 -> 1/2\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec1 = ConvBlock(128 + 64, 64) \n",
    "\n",
    "        # Up 2: 1/2 -> Full\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec2 = ConvBlock(64 + 32, 32) \n",
    "\n",
    "        self.final = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "        # self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(128, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(64, 4)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        e1 = self.enc1(x)       # [B, 32, H, W]\n",
    "        p1 = self.pool1(e1)     # [B, 32, H/2, W/2]\n",
    "        \n",
    "        e2 = self.enc2(p1)      # [B, 64, H/2, W/2]\n",
    "        p2 = self.pool2(e2)     # [B, 64, H/4, W/4]\n",
    "\n",
    "        b = self.bottleneck(p2) # [B, 128, H/4, W/4]\n",
    "\n",
    "        d1 = self.up1(b)        # [B, 128, H/2, W/2]\n",
    "        d1 = torch.cat([d1, e2], dim=1) \n",
    "        d1 = self.dec1(d1)      # [B, 64, H/2, W/2]\n",
    "\n",
    "        d2 = self.up2(d1)       # [B, 64, H, W]\n",
    "        d2 = torch.cat([d2, e1], dim=1)\n",
    "        d2 = self.dec2(d2)      # [B, 32, H, W]\n",
    "\n",
    "        mask = self.final(d2) # [B, 1, H, W]\n",
    "        \n",
    "        # v_logits = self.classifier(self.gap(b)) # [B, 4]\n",
    "\n",
    "        # v = 1 ALWAYS\n",
    "        # for i in range(8):\n",
    "        #     v_logits[i][1] = 1.0\n",
    "        #     v_logits[i][0] = 0.0\n",
    "        #     v_logits[i][2] = 0.0\n",
    "        #     v_logits[i][3] = 0.0\n",
    "        v_logits = torch.zeros(b.size(0), 4, device=b.device)\n",
    "        v_logits[:, 1] = 1.0\n",
    "\n",
    "        return mask, v_logits\n",
    "        \n",
    "    def save_weights(self, path=\"model.pth\"):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_weights(self, path, device=None):\n",
    "        self.load_state_dict(torch.load(path, map_location=device, weights_only=False))\n",
    "\n",
    "    def load(self, link, device):\n",
    "        # todo: add code for loading model here\n",
    "        print('Running stub for loading model ...')\n",
    "        output = 'model.pth'\n",
    "        gdown.download(link, output, quiet=False)\n",
    "        self.load_weights(output, device)\n",
    "        print('Loading model done.')\n",
    "\n",
    "    def predict_on_batch(self, batch: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Input: [B, H, W, C]  (C = 3 * stack_s)\n",
    "        Output:[B, H, W]\n",
    "        \"\"\"\n",
    "        # to tensor + layout\n",
    "        x = torch.from_numpy(batch).to(next(self.parameters()).device)\n",
    "        x = x.permute(0, 3, 1, 2).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mask, v_logits = self(x)\n",
    "\n",
    "        mask = torch.sigmoid(mask) \n",
    "        mask = mask.squeeze(1).cpu().numpy()\n",
    "        v = torch.argmax(v_logits, dim=1).cpu().numpy()\n",
    "\n",
    "        #############\n",
    "        # v = 1 ####### !!!!!!!!!!!!!!!!!\n",
    "        #############\n",
    "\n",
    "        return mask, v\n",
    "\n",
    "    def mask_to_coords(self, mask):\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = torch.relu(mask - 0.15) # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            mask = mask.cpu().detach().numpy()\n",
    "        else:\n",
    "            mask = (mask > 0.15).astype(np.float32)\n",
    "        \n",
    "        if mask.ndim == 3: mask = mask[0]\n",
    "        \n",
    "        if mask.max() < self.threshold:\n",
    "            return None\n",
    "\n",
    "        total_mass = mask.sum()\n",
    "        if total_mass <= 0: return None\n",
    "        \n",
    "        # coordinate grids\n",
    "        grid_y, grid_x = np.indices(mask.shape)\n",
    "        \n",
    "        #  Centroid\n",
    "        x = (grid_x * mask).sum() / total_mass\n",
    "        y = (grid_y * mask).sum() / total_mass\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "        \n",
    "        \n",
    "    def _predict_prob_on_clip(self, clip: np.ndarray) -> np.ndarray:\n",
    "        print('doing predictions')\n",
    "        n_frames = clip.shape[0]\n",
    "        print(f'stack_s: {stack_s} n_frames: {n_frames}')\n",
    "        # --- get stacks ---\n",
    "        stacks = []\n",
    "        for i in range(n_frames - self.stack_s + 1):\n",
    "            stack = clip[i : i + self.stack_s, ...]\n",
    "            stack = np.squeeze(np.split(stack, self.stack_s, axis=0))\n",
    "            stack = np.concatenate(stack, axis=-1)\n",
    "            stacks.append(stack)\n",
    "        # --- round to batch size ---\n",
    "        add_stacks = 0\n",
    "        while len(stacks) % self.batch_s != 0:\n",
    "            stacks.append(stacks[-1])\n",
    "            add_stacks += 1\n",
    "        # --- group into batches ---\n",
    "        batches = []\n",
    "        for i in range(len(stacks) // self.batch_s):\n",
    "            batch = np.stack(stacks[i * self.batch_s : (i + 1) * self.batch_s])\n",
    "            batches.append(batch)\n",
    "        stacks.clear()\n",
    "        # --- perform predictions ---\n",
    "        predictions = []\n",
    "        pred_v = []\n",
    "        for batch in batches:\n",
    "            pred_mask, v = self.predict_on_batch(batch)\n",
    "            predictions.append(pred_mask)\n",
    "            pred_v.append(v)\n",
    "        # --- crop back to source length ---\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        if (add_stacks > 0):\n",
    "            predictions = predictions[:-add_stacks, ...]\n",
    "        batches.clear()\n",
    "        # --- add (stack_s - 1) null frames at the begining ---\n",
    "        start_frames = np.zeros((stack_s - 1, predictions.shape[1], predictions.shape[2]), dtype=np.float32)\n",
    "        predictions = np.concatenate((start_frames, predictions), axis=0)\n",
    "        pred_v = np.concatenate(pred_v, axis=0)\n",
    "        pad_v = np.zeros(self.stack_s - 1, dtype=np.int64)\n",
    "        pred_v = np.concatenate([pad_v, pred_v])\n",
    "        print('predictions are made')        \n",
    "        return predictions, pred_v\n",
    "\n",
    "    def get_labels_from_prediction(self, pred_prob, v, upscale_coords):\n",
    "        n_frames = pred_prob.shape[0]\n",
    "        coords = np.zeros((n_frames, 4), dtype=np.float32)\n",
    "\n",
    "        for i in range(n_frames):\n",
    "            xy = self.mask_to_coords(pred_prob[i])\n",
    "\n",
    "            if xy is None:\n",
    "                coords[i] = [0, 0, 0, 0]\n",
    "                continue\n",
    "\n",
    "            x, y = xy\n",
    "\n",
    "            if upscale_coords:\n",
    "                x *= 2\n",
    "                y *= 2\n",
    "\n",
    "            coords[i] = [1, x, y, 0]\n",
    "\n",
    "        return coords\n",
    "\n",
    "\n",
    "    def predict(self, clip: np.ndarray, upscale_coords=True):\n",
    "        pred_masks, pred_vs = self._predict_prob_on_clip(clip)\n",
    "        labels_pr = self.get_labels_from_prediction(pred_masks, pred_vs, upscale_coords)\n",
    "        return labels_pr, pred_masks\n",
    "\n",
    "    def test(self, data_path: Path, games: List[int], do_visualization=False, test_name='test'):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            game_clip_pairs = get_game_clip_pairs(data_path, games)\n",
    "            SIBATRACC_vals = []\n",
    "            for game, clip in game_clip_pairs:\n",
    "                data = load_clip_data(data_path, game, clip, downscale=self.downscale)\n",
    "                if do_visualization:\n",
    "                    data_full = load_clip_data(data_path, game, clip, downscale=False) if self.downscale else data\n",
    "                labels_gt = load_clip_labels(data_path, game, clip, downscale=False)\n",
    "                labels_pr, prob_pr = self.predict(data, upscale_coords=self.downscale)\n",
    "                SIBATRACC_per_frame, SIBATRACC_total = Metrics.evaluate_predictions(labels_gt, labels_pr)\n",
    "                SIBATRACC_vals.append(SIBATRACC_total)\n",
    "                if do_visualization:\n",
    "                    visualize_prediction(data_full, labels_pr, self.out_path, f'{test_name}_g{game}_c{clip}', SIBATRACC_per_frame)\n",
    "                    visualize_prob(data, prob_pr, self.out_path, f'{test_name}_g{game}_c{clip}')\n",
    "                    del data_full\n",
    "                del data, labels_gt, labels_pr, prob_pr\n",
    "                gc.collect()\n",
    "            SIBATRACC_final = sum(SIBATRACC_vals) / len(SIBATRACC_vals)\n",
    "        return SIBATRACC_final\n",
    "\n",
    "    def train_on_data(self,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        max_epochs=10,\n",
    "        patience=5,\n",
    "        lr = 3e-4,\n",
    "        num_of_steps = 200):\n",
    "        \n",
    "        # todo: implement model training here\n",
    "        print('Running stub for training model...')\n",
    "\n",
    "        optimazer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        # criterion = DiceBCELoss()\n",
    "        criterion_v = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        best_loss = 9999.0\n",
    "        epochs_no_imp = 0\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        # img, mask, _ = train_gen.get_random_batch(1)\n",
    "\n",
    "        # img = torch.from_numpy(img).permute(0,3,1,2).float().to(device)\n",
    "        # mask = torch.from_numpy(mask).unsqueeze(1).float().to(device)\n",
    "\n",
    "        # for i in range(200):\n",
    "        #     pred, _ = self(img)\n",
    "        #     loss = F.mse_loss(torch.sigmoid(pred), mask)\n",
    "        #     loss.backward()\n",
    "        #     optimazer.step()\n",
    "        #     optimazer.zero_grad()\n",
    "\n",
    "        # print(loss.item())\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            i = 0\n",
    "\n",
    "            # train_loop = notebook.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{max_epochs}\", leave=False)\n",
    "            train_iter = iter(train_loader)\n",
    "            bar = tqdm(range(num_of_steps))\n",
    "            total_loss = 0\n",
    "            alt_total = 0\n",
    "\n",
    "            for step in bar:\n",
    "                if i >= num_of_steps:\n",
    "                    break\n",
    "\n",
    "                    \n",
    "\n",
    "                clip, gt_mask, gt_v = next(train_iter)\n",
    "                if clip.shape[0] < 2:\n",
    "                    continue\n",
    "                \n",
    "                optimazer.zero_grad()\n",
    "                clip, gt_mask, gt_v = clip.to(device), gt_mask.to(device), gt_v.to(device)\n",
    "                # if step == 0:\n",
    "                #     tqdm.write(f\"Max value in input clip: {clip.max()}\")\n",
    "                #     tqdm.write(f\"Max value in GT mask: {gt_mask.max()}\")\n",
    "    \n",
    "                pred_mask, v_logits = self(clip)\n",
    "                prob = torch.sigmoid(pred_mask)\n",
    "                \n",
    "\n",
    "                if gt_mask.dim() == 3:\n",
    "                    gt_mask = gt_mask.unsqueeze(1)\n",
    "\n",
    "                area_w = min(0.06, (epoch / 3) * 0.06)\n",
    "                loss = 0.3*DiceLoss()(prob, gt_mask) + 0.67*FocalLoss()(pred_mask, gt_mask) + area_w * area_regularization(prob)\n",
    "                # alt_total += criterion(pred_mask, gt_mask)\n",
    "                # pred_v = torch.argmax(v_logits)\n",
    "\n",
    "                # v_loss = criterion_v(v_logits, gt_v.long())\n",
    "\n",
    "                # loss +=  v_loss * 0.0\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "                optimazer.step()\n",
    "\n",
    "            tqdm.write(f'Epoch is {epoch} Total train loss is {total_loss / num_of_steps :.4f}')\n",
    "            tqdm.write(f'DICE + Focal loss {alt_total / num_of_steps}')\n",
    "            total_loss = 0\n",
    "            alt_total = 0\n",
    "\n",
    "            val_iter = iter(val_loader)\n",
    "            vbar = tqdm(range(num_of_steps // 4))\n",
    "            for step in vbar:\n",
    "                if i >= num_of_steps:\n",
    "                    break\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    clip, gt_mask, gt_v = next(val_iter)\n",
    "                    if clip.shape[0] < 2:\n",
    "                        continue\n",
    "            \n",
    "                    optimazer.zero_grad()\n",
    "                    clip, gt_mask, gt_v = clip.to(device), gt_mask.to(device), gt_v.to(device)\n",
    "                    pred_mask, v_logits = self(clip)\n",
    "                    prob = torch.sigmoid(pred_mask)\n",
    "                    \n",
    "\n",
    "                    if step == 10:\n",
    "                        plt.imshow(pred_mask[0].cpu().squeeze(0), cmap='plasma')\n",
    "                        plt.show()\n",
    "                    if step == 9:\n",
    "                        plt.close('all')\n",
    "\n",
    "                    if gt_mask.dim() == 3:\n",
    "                        gt_mask = gt_mask.unsqueeze(1)\n",
    "\n",
    "                    # loss =  0.7 * F.mse_loss(pred_mask, gt_mask) + 0.3 * F.l1_loss(pred_mask, gt_mask)\n",
    "                    loss = 0.3*DiceLoss()(prob, gt_mask) + 0.67*FocalLoss()(pred_mask, gt_mask) + area_w * area_regularization(prob)\n",
    "                    # pred_v = torch.argmax(v_logits)\n",
    "\n",
    "                    # v_loss = criterion_v(v_logits, gt_v.long())\n",
    "\n",
    "                    total_loss += loss\n",
    "\n",
    "            tqdm.write(f'Total val loss is {total_loss / (num_of_steps // 4) :.4f} \\n')\n",
    "            if total_loss < best_loss:\n",
    "                best_loss = total_loss\n",
    "                epoch_no_imp = 0\n",
    "                self.save_weights()\n",
    "            else:\n",
    "                epoch_no_imp += 1\n",
    "            \n",
    "            if epoch_no_imp > patience:\n",
    "                tqdm.write(f\"patince is over. training is over. smallestt loss is {best_loss // (num_of_steps // 4) :.4f}\")\n",
    "                break\n",
    "                \n",
    "            \n",
    "            i += 1\n",
    "        self.save_weights('last_epoch.pth')\n",
    "        \n",
    "        print('training done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример пайплайна для обучения модели:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глобальные переменные !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:24:23.417936Z",
     "iopub.status.busy": "2025-12-21T15:24:23.417560Z",
     "iopub.status.idle": "2025-12-21T15:24:23.443110Z",
     "shell.execute_reply": "2025-12-21T15:24:23.441783Z",
     "shell.execute_reply.started": "2025-12-21T15:24:23.417913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MAX_EPOCHS = 25\n",
    "STACK_S = 8\n",
    "BATCH_S = 8\n",
    "THRESHOLD = 0.02\n",
    "NUM_OF_STEPS = 300\n",
    "PATIENCE = 7\n",
    "batch_s = BATCH_S\n",
    "stack_s = STACK_S\n",
    "downscale = True\n",
    "output_path = prepare_experiment(Path('out_path'))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:24:23.444488Z",
     "iopub.status.busy": "2025-12-21T15:24:23.444201Z",
     "iopub.status.idle": "2025-12-21T15:24:23.518047Z",
     "shell.execute_reply": "2025-12-21T15:24:23.516799Z",
     "shell.execute_reply.started": "2025-12-21T15:24:23.444464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = SuperTrackingModel(batch_s, stack_s, out_path=output_path, downscale=downscale, threshold = THRESHOLD)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:24:23.519523Z",
     "iopub.status.busy": "2025-12-21T15:24:23.519179Z",
     "iopub.status.idle": "2025-12-21T15:24:43.630751Z",
     "shell.execute_reply": "2025-12-21T15:24:43.629788Z",
     "shell.execute_reply.started": "2025-12-21T15:24:23.519493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(Path('data/train'), [1, 2, 3], stack_s=stack_s, downscale=True, pool_s=10, pool_update_s=4, quiet=False)\n",
    "val_gen = DataGenerator(Path('data/train'), [4], stack_s=stack_s, downscale=True, pool_s=4, pool_update_s=2, quiet=False)\n",
    "\n",
    "train_l = DataLoader(PyTorchWrap(train_gen, mode=\"train\"), batch_size=BATCH_S, shuffle=False)\n",
    "val_l = DataLoader(PyTorchWrap(val_gen), batch_size=BATCH_S, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# img, mask, _ = train_gen.get_random_batch(1)\n",
    "\n",
    "# img = torch.from_numpy(img).permute(0,3,1,2).float().to(device)\n",
    "# mask = torch.from_numpy(mask).unsqueeze(1).float().to(device)\n",
    "\n",
    "# optimazer = torch.optim.Adam(lr=lr)\n",
    "# for i in range(200):\n",
    "#     pred, _ = model(img)\n",
    "#     loss = F.mse_loss(torch.sigmoid(pred), mask)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "# print(loss.item())\n",
    "\n",
    "# if step % 50 == 0:\n",
    "#     print(f\"step {step} | loss {loss.item():.6f}\")\n",
    "\n",
    "# pred = model(img).detach().cpu()[0,0]\n",
    "# gt = gt.cpu()[0,0]\n",
    "\n",
    "# plt.figure(figsize=(12,4))\n",
    "# plt.subplot(1,3,1); plt.title(\"GT\"); plt.imshow(gt); plt.colorbar()\n",
    "# plt.subplot(1,3,2); plt.title(\"Pred\"); plt.imshow(pred); plt.colorbar()\n",
    "# plt.subplot(1,3,3); plt.title(\"Diff\"); plt.imshow(pred-gt); plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames, mask, _ = train_gen.get_random_stack()\n",
    "# plt.imshow(mask)\n",
    "# plt.scatter([mask.argmax()%mask.shape[1]],\n",
    "#             [mask.argmax()//mask.shape[1]], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = np.zeros((1, 360, 640, 3)) # Dummy frame\n",
    "# test_label = np.array([[1, 100, 100]]) # Visible at 100, 100\n",
    "# generated_mask = create_masks(test_data, test_label, False)\n",
    "\n",
    "# print(f\"Mask max value: {generated_mask.max()}\") # Should be 1.0\n",
    "# plt.imshow(generated_mask[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:24:43.631980Z",
     "iopub.status.busy": "2025-12-21T15:24:43.631722Z",
     "iopub.status.idle": "2025-12-21T15:26:18.192398Z",
     "shell.execute_reply": "2025-12-21T15:26:18.191363Z",
     "shell.execute_reply.started": "2025-12-21T15:24:43.631954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.train_on_data(train_l, val_l, device= device, max_epochs=MAX_EPOCHS, num_of_steps=NUM_OF_STEPS, patience= PATIENCE, lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример пайплайна для тестирования обученной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    new_model = SuperTrackingModel(batch_s, stack_s, out_path=output_path, downscale=downscale, threshold = THRESHOLD)\n",
    "    new_model.load_weights(\"best_model_1.pth\", device)\n",
    "else:\n",
    "    new_model = model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T15:26:18.193874Z",
     "iopub.status.busy": "2025-12-21T15:26:18.193454Z",
     "iopub.status.idle": "2025-12-21T15:29:18.339036Z",
     "shell.execute_reply": "2025-12-21T15:29:18.337804Z",
     "shell.execute_reply.started": "2025-12-21T15:26:18.193849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# new_model = SuperTrackingModel(batch_s, stack_s, out_path=output_path, downscale=downscale)\n",
    "# new_model.load()\n",
    "sibatracc_final = new_model.test(Path('data/test'), [1,], do_visualization=True, test_name='test')\n",
    "print(f'SiBaTrAcc final value: {sibatracc_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест модели \n",
    "что бы загрузка заработала нужно запустить все клетки кода от помеченной как Глобальные Переменные и выше и потом запустить код ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stub for loading model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1L_R9XNFS906SvJ9CkI-mwpH9USFE7yDa\n",
      "To: d:\\tracking_model\\model.pth\n",
      "100%|██████████| 2.11M/2.11M [00:00<00:00, 17.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "link = 'https://drive.google.com/uc?id=1L_R9XNFS906SvJ9CkI-mwpH9USFE7yDa'\n",
    "test_model = new_model = SuperTrackingModel(batch_s, stack_s, out_path=output_path, downscale=downscale, threshold = THRESHOLD)\n",
    "test_model.to(device)\n",
    "\n",
    "test_model.load(link, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading clip data (game 1, clip 1) downscaled\n",
      "loading clip data (game 1, clip 1) \n",
      "loading clip labels (game 1, clip 1)\n",
      "doing predictions\n",
      "stack_s: 8 n_frames: 361\n",
      "predictions are made\n",
      "perfoming clip visualization\n",
      "loading clip data (game 1, clip 2) downscaled\n",
      "loading clip data (game 1, clip 2) \n",
      "loading clip labels (game 1, clip 2)\n",
      "doing predictions\n",
      "stack_s: 8 n_frames: 199\n",
      "predictions are made\n",
      "perfoming clip visualization\n",
      "loading clip data (game 1, clip 3) downscaled\n",
      "loading clip data (game 1, clip 3) \n",
      "loading clip labels (game 1, clip 3)\n",
      "doing predictions\n",
      "stack_s: 8 n_frames: 36\n",
      "predictions are made\n",
      "perfoming clip visualization\n",
      "loading clip data (game 1, clip 4) downscaled\n",
      "loading clip data (game 1, clip 4) \n",
      "loading clip labels (game 1, clip 4)\n",
      "doing predictions\n",
      "stack_s: 8 n_frames: 45\n",
      "predictions are made\n",
      "perfoming clip visualization\n",
      "loading clip data (game 1, clip 5) downscaled\n",
      "loading clip data (game 1, clip 5) \n",
      "loading clip labels (game 1, clip 5)\n",
      "doing predictions\n",
      "stack_s: 8 n_frames: 196\n",
      "predictions are made\n",
      "perfoming clip visualization\n",
      "loading clip data (game 1, clip 6) downscaled\n",
      "loading clip data (game 1, clip 6) \n",
      "loading clip labels (game 1, clip 6)\n",
      "doing predictions\n",
      "stack_s: 8 n_frames: 551\n",
      "predictions are made\n",
      "perfoming clip visualization\n",
      "loading clip data (game 1, clip 7) downscaled\n",
      "loading clip data (game 1, clip 7) \n",
      "loading clip labels (game 1, clip 7)\n",
      "doing predictions\n",
      "stack_s: 8 n_frames: 189\n",
      "predictions are made\n",
      "perfoming clip visualization\n",
      "loading clip data (game 1, clip 8) downscaled\n",
      "loading clip data (game 1, clip 8) \n",
      "loading clip labels (game 1, clip 8)\n",
      "doing predictions\n",
      "stack_s: 8 n_frames: 645\n",
      "predictions are made\n",
      "perfoming clip visualization\n",
      "SiBaTrAcc final value: 0.5641243163674312\n"
     ]
    }
   ],
   "source": [
    "sibatracc_final = test_model.test(Path('data/test'), [1,], do_visualization=True, test_name='test')\n",
    "print(f'SiBaTrAcc final value: {sibatracc_final}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1250524,
     "sourceId": 2085688,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
